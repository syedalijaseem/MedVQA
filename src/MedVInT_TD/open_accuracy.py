import pandas as pd
import argparse
import difflib
from tqdm import tqdm
import os

def str_similarity(str1, str2):
    """Calculates similarity using SequenceMatcher, as mentioned in the paper."""
    return difflib.SequenceMatcher(None, str1, str2).ratio()

def find_most_similar(target_str, str_list):
    """
    Given a target string and a list of strings, returns the string from the list
    that is most similar to the target.
    """
    most_similar_str = None
    highest_similarity = -1

    for s in str_list:
        similarity = str_similarity(target_str, s)
        if similarity > highest_similarity:
            most_similar_str = s
            highest_similarity = similarity
            
    return most_similar_str

def main():
    parser = argparse.ArgumentParser(description="Calculate open-ended accuracy and save detailed results.")
    parser.add_argument("csv_file", type=str, help="Path to the input CSV file generated by test_VQA_RAD.py")
    args = parser.parse_args()

    try:
        df = pd.read_csv(args.csv_file)
    except FileNotFoundError:
        print(f"Error: File not found at {args.csv_file}")
        return

    if not {'Question', 'Prediction', 'Label', 'Image_Name'}.issubset(df.columns):
        print("Error: CSV file must contain 'Question', 'Prediction', 'Label', and 'Image_Name' columns.")
        return

    # Step 1: Create a list of all unique ground-truth answers.
    unique_labels = list(df['Label'].str.lower().unique())
    print(f"Created a set of {len(unique_labels)} unique answers from the test set.")

    correct_predictions = 0
    total_samples = len(df)
    results_data = []

    for index, row in tqdm(df.iterrows(), total=total_samples, desc="Calculating Accuracy"):
        question = row['Question']
        raw_prediction = str(row['Prediction'])
        true_label = str(row['Label'])
        image_name = row['Image_Name']

        # Step 2: Normalize the prediction by finding the most similar answer in the unique label set.
        normalized_prediction = find_most_similar(raw_prediction.lower(), unique_labels)

        # Step 3: Compare for accuracy and store results.
        is_correct = (normalized_prediction == true_label.lower())
        if is_correct:
            correct_predictions += 1
        
        results_data.append({
            "Question": question,
            "Label": true_label,
            "Raw_Prediction": raw_prediction,
            "Normalized_Prediction": normalized_prediction,
            "Is_Correct": is_correct,
            "Image_Name": image_name
        })
    
    # --- Save Detailed Results to a New CSV ---
    results_df = pd.DataFrame(results_data)
    output_filename = f"accuracy_details_{os.path.basename(args.csv_file)}"
    results_df.to_csv(output_filename, index=False)
    print(f"\nSaved detailed accuracy results to: {output_filename}")

    # --- Print Final Score ---
    if total_samples > 0:
        accuracy = (correct_predictions / total_samples) * 100
        print(f"\nFinal Open-Ended Accuracy: {accuracy:.2f}%")
        print(f"({correct_predictions} correct out of {total_samples} samples)")
    else:
        print("No samples found in the CSV file.")

if __name__ == "__main__":
    main()