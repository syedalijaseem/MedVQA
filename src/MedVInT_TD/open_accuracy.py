import pandas as pd
import argparse
import difflib
from tqdm import tqdm

def str_similarity(str1, str2):
    """Calculates similarity using SequenceMatcher, as mentioned in the paper."""
    return difflib.SequenceMatcher(None, str1, str2).ratio()

def find_most_similar(target_str, str_list):
    """
    Given a target string and a list of strings, returns the string from the list
    that is most similar to the target.
    """
    most_similar_str = None
    highest_similarity = -1

    for s in str_list:
        similarity = str_similarity(target_str, s)
        if similarity > highest_similarity:
            most_similar_str = s
            highest_similarity = similarity
            
    return most_similar_str

def main():
    parser = argparse.ArgumentParser(description="Calculate open-ended accuracy from a results CSV using the author's confirmed method.")
    parser.add_argument("csv_file", type=str, help="Path to the input CSV file generated by test_VQA_RAD.py")
    args = parser.parse_args()

    try:
        df = pd.read_csv(args.csv_file)
    except FileNotFoundError:
        print(f"Error: File not found at {args.csv_file}")
        return

    if not {'Prediction', 'Label'}.issubset(df.columns):
        print("Error: CSV file must contain 'Prediction' and 'Label' columns.")
        return

    # Step 1: Create a list of all unique ground-truth answers from the test set.
    unique_labels = list(df['Label'].str.lower().unique())
    print(f"Created a set of {len(unique_labels)} unique answers from the test set.")

    correct_predictions = 0
    total_samples = len(df)

    # Use tqdm for a progress bar
    for index, row in tqdm(df.iterrows(), total=total_samples, desc="Calculating Accuracy"):
        prediction = str(row['Prediction']).lower()
        true_label = str(row['Label']).lower()

        # Step 2: For each prediction, find the most similar answer in the unique label set.
        normalized_prediction = find_most_similar(prediction, unique_labels)

        # Step 3: Compare the normalized prediction to the actual ground-truth label.
        if normalized_prediction == true_label:
            correct_predictions += 1
    
    if total_samples > 0:
        accuracy = (correct_predictions / total_samples) * 100
        print(f"\nFinal Open-Ended Accuracy: {accuracy:.2f}%")
        print(f"({correct_predictions} correct out of {total_samples} samples)")
    else:
        print("No samples found in the CSV file.")

if __name__ == "__main__":
    main()